# ğŸ“˜ Chapter 2: Scaling with RAG

In the previous chapter, we established that our initial **two-stage LLM pipeline** is fundamentally **not scalable**.
When handling hundreds or thousands of candidates, the **volume of data** overwhelms the Large Language Modelâ€™s (LLM) **context window** â€” its maximum input limit.

To overcome this, we introduce **Retrieval-Augmented Generation (RAG)** â€” an advanced architectural pattern that performs **data retrieval and filtering** *before* the LLM even processes the query.

---

## ğŸ§© 2.1 What is Retrieval-Augmented Generation (RAG)?

**RAG** combines a **traditional information retrieval system** (such as a vector or specialized database) with the **reasoning capabilities of an LLM**.

Instead of sending **all available data** to the LLM (a brute-force approach), RAG ensures that **only the most relevant snippets** are sent for reasoning.

> **Analogy:**
> Think of RAG as a *hyper-efficient librarian* who filters millions of books and hands the researcher **only the three pages they actually need**.

---

## âš™ï¸ 2.2 The Four Pillars of the RAG Pipeline

Implementing RAG requires four key technological components working together in harmony.

---

### ğŸ§± 1. Chunking (Segmentation)

**Definition:**
Breaking large documents (e.g., 2-page resumes) into smaller, manageable pieces of text called **chunks** (typically around 200 words each).

**Necessity:**
Chunks are small enough to:

* Be represented accurately by a single vector.
* Fit efficiently within the LLMâ€™s context window later.

**Application:**
Our **Node.js backend** will perform chunking on the raw resume text after parsing it from the uploaded PDF.

---

### ğŸ”¢ 2. Embedding (Quantification)

**Definition:**
Converting each text chunk into a **mathematical vector** â€” a numerical list such as `[0.12, -0.45, 0.99, ...]`.

**Necessity:**
Computers canâ€™t understand text directly, but they can work with numbers.
In this *vector space*, similar meanings appear close together â€” enabling **semantic search**.

**Application:**
Our **Node.js backend** will use an **Embedding Model** (e.g., OpenAI or local sentence-transformer) to convert every resume chunk into a vector.

---

### ğŸ—„ï¸ 3. Vector Database (Storage)

**Definition:**
A specialized database optimized for storing and searching high-dimensional vectors â€” enabling efficient similarity matching among millions of entries.

**Necessity:**
Traditional databases like MySQL or MongoDB excel at structured data (names, dates) but struggle with vector search.
A **Vector Database** is designed specifically for **semantic retrieval**.

**Application:**
We will use **PostgreSQL** enhanced with the **pgvector** extension to store all candidate resume vectors efficiently.

---

### ğŸ” 4. Similarity Search (Retrieval)

**Definition:**
When a query (e.g., â€œFind candidates with AWS and leadership experienceâ€) is received, the query text is converted into a **query vector**.
This vector is compared against all stored resume vectors to find those **mathematically closest** â€” meaning **most semantically relevant**.

**Necessity:**
This process instantly identifies the relevant resume segments (â€œthe needlesâ€) that answer the HR teamâ€™s query.

**Application:**
The **retrieved text chunks** and their associated **candidate IDs** are packaged and passed to the LLM for final ranking and reasoning.

---

## ğŸ§  2.3 RAG: The Scalable Ranking Engine

With RAG in place, when the HR Admin asks the LLM to rank candidates, the LLM no longer processes every resume.
Instead, it receives only the **top 10â€“20 most relevant snippets**, already pre-filtered by **vector search**.

This provides multiple advantages:

* âš¡ **Faster Response Time** â€” Smaller input size â†’ quicker reasoning.
* ğŸ’¸ **Lower Cost** â€” Fewer tokens â†’ cheaper LLM usage.
* ğŸ“ˆ **Scalable Performance** â€” Works smoothly even with **thousands of applicants**.
* ğŸ§© **Higher Relevance** â€” LLM focuses only on contextually relevant information.

---

## ğŸš€ Next Chapter: The Full Tech Stack and Intelligence Flow

In **Chapter 3**, weâ€™ll define the complete **technology stack** for the AI-Powered Recruiter:

* ğŸ–¥ï¸ **Backend:** Node.js
* ğŸ—ƒï¸ **Database:** PostgreSQL + pgvector
* ğŸ§  **AI Layer:** Local AI (for instant UX feedback) + Cloud-based RAG (for scalable ranking)

Weâ€™ll also map how **local inference** and **cloud orchestration** blend to deliver a seamless, intelligent recruitment workflow.

---
